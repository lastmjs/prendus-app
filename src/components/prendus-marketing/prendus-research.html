<link rel="import" href="../../bower_components/polymer/polymer.html">
<link rel="import" href="../prendus-ui/prendus-styles.html">


<dom-module id="prendus-research">
    <template>
      <style include ="prendus-styles">
    .page {
      display: flex;
      align-items: center;
      justify-content: center;
      flex-wrap: wrap;
    }
    .main-paragraph
    {
      width: 60vw;
    }
    .sub{
      padding-top: 10vh;
      display: flex;
      align-items: center;
    }
    .text {
      width: 50vw;
    }
    .image {
      width: 50vw;
      justify-content: center;
      align-items: center;
      align-self: center;
    }
    h1 {
      text-align: center;
    }
      </style>
      <div class="page">
        <div class="main-paragraph">
          <h1>Research Background</h1>
          <p>
            Prendus is	young,	but	the	principles	underlying	its	success	are	not.	For	40	years,	many	instructors–from	K-
            121 through	higher	education2– have	assigned	their	students	to	write	practice	or	assessment	questions.3
            In	some	cases,	instructors	use	student-generated	assessment	items	on	exams	to	motivate	students	to
            create	and	review	good	questions.	In	other	cases,	students	are	required	to	evaluate	their	classmates’
            questions	for	quality.	In	addition,	sometimes	instructors	compile	students’	questions	to	provide	them
            extra	practice	problems.
          </p>
          <p>
            A	number	of	educational	theories	suggest	why
            creating	questions	might	be	a	valuable
            pedagogical	activity	for	students:	active	text
            processing,	review	and	elaboration,	metacognition,
            socio-cognitive	theories,	higherorder
            thinking,	and	generative	learning	theory.4
            Researchers	have	found	that	writing	assessment
            questions	has	a	positive	effect	on	student
            achievement	and	problem-solving	abilities.	In
            other	words,	students	who	create	questions
            perform	better	on	exams	than	others	who	reread
            passages	or	practice	additional	problems.5
          </p>
          <p>
            Research	has	also	shown	that	students	create	better	questions	if	they	receive	training,	or	are	guided
            through	the	process	by	scaffolding.6 Conversely,	if	no	training	or	scaffolding	is	provided,	students	usually
            lack	the	motivation	or	necessary	skills	to	create	high-quality	questions,	which	results	in	low	question
            quality.
          </p>
          <p>
            Despite	all	of	the	work	and	promising	results	generated	from	research	on	this	concept,	there	are	only	a
            handful	of	online	platforms	that	facilitate	this	type	of	instructional	strategy.	Peerwise,	AICHe	Concept
            Warehouse,	StudySieve,	and	Quizlet are	all	online	platforms	that	facilitate	student	question	creation,
            archival,	and	reuse,	but	they	each	fail	to	capitalize	on	the	process’s	potential.
          </p>
          <p>
            For	example,	some	provide	just-in-time	training	as	students	create	questions,	which	requires	further
            outside	training	to	excel.	None	provide	sufficient	training	or	scaffolding	in	the	question	creation	process
            to	enable	students	to	create	high-quality	questions.	In	addition,	while	peer	evaluation	can	be	used	to
            ensure	that	only	high-quality	questions	remain	in	the	class	repository,	only	Peerwise	and	StudySieve	use
            some	kind	of	question	evaluation	activity.
          </p>
          <p>
            As	for	question	reusability,	one	method	is	to	tag	questions	with	an	appropriate	learning	outcome.	This
            helps	faculty	find	questions	to	incorporate	in	their	courses,	by	searching	for	relevant	learning	outcomes.
            Peerwise	allows	users	to	generate	their	own	tags	related	to	learning	objectives,	but	does	not	provide	a
            top-down	mandated	or	curated	list	of	learning	outcomes,	which	means	that	instructors	may	fail	to	find
            high-quality	questions	because	of	different	wording	choices.
          </p>
          <p>
            One	researcher	conducted	a	literature	review	on	student	generated	assessment	and	concluded	that	more
            research	was	needed	on	identifying	what	types	of	scaffolding	were	needed	to	ensure	students	were	able
            and	motivated	to	create	high	quality	questions.7
          </p>
          <p>
            To	address	the	gap	in	student-generated	assessment	platforms,	we	have	developed	a	platform	called
            Prendus	that	scaffolds	the	question	creation	process,	provides	opportunities	for	students	to	evaluate	one
            another’s	questions,	and	allows	students	to	tag	their	questions	with	learning	objectives	approved	by	their
            instructors.
          </p>
          <p>
            To	summarize,	the	research	on	student	generated	assessments	is	very	promising	and	provides	some
            evidence	that	(1)	having	students	create	questions	increases	student	achievement	and	(2)	students	are
            able	to	create	high-quality	questions,	if	they	receive	high-quality	training	or	scaffolding.
          </p>
          <h1>Notes</h1>
          <p>
            1 Yu	et	al.,	2013;	King,	1991;	King	&	Rosenshine,	1993
            2 King,	1992;	Foote,	1998;	van	Blerkom et	al.,	2006
            3 Foos,	1989;	Frase &	Schwartz,	1975
            4 Song,	2016
            5 Foos,	1989;	Foos,	Mora,	&	Tkacz,	1994;	Frase &	Schwartz,	1975;	Carroll,	2001;	Van	Blerkom,	Van	Blerkom,	&
            Bertsch,	2006;	Kerkman,	Kellison,	Pinon,	Schmidt,	&	Lewis,	1994;	Luxton-Reilly,	Bertinshaw,	Denny,	Plimmer,	&
            Sheehan,	2012;	Schullo-Feulner et	al.,	2014
            6 Bates,	Galloway,	Rise,	&	Homer,	2014;	Yu,	2009;	King,	1992;	Denny,	Luxton-Reilly,	&	Simon,	2009;	Hardy	et	al.
            2014;	Bates,	Galloway,	Riise,	&	Homer,	2014;	Denny,	Luxton-Reilly,	Hamer,	&	Purchase,	2009
            7 Song,	2016
          </p>
          <h1>References	on	Student	Question	Quality</h1>
          <p>
            Bates,	S.	P.,	Galloway,	R.	K.,	Rise,	J.,	&	Homer,	D.	(2014).	Assessing	the	quality	of	a	student-generated
            question	repository.	Physical	Review	Special	Topics-Physics	Education	Research,	10(2),	020105.
            Denny,	P.,	Luxton-Reilly,	A.,	Hamer,	J.,	&	Purchase,	H.	(2009,	July).	Coverage	of	course	topics	in	a	student
            generated	MCQ	repository.	In	ACM	SIGCSE	Bulletin (Vol.	41,	No.	3,	pp.	11-15).	ACM.
            Denny,	P.,	Luxton-Reilly,	A.,	&	Simon,	B.	(2009,	January).	Quality	of	student	contributed	questions	using
            PeerWise.	In	Proceedings	of	the	Eleventh	Australasian	Conference	on	Computing	Education,	95.
            Australian	Computer	Society,	Inc..
            Hardy,	J.,	Bates,	S.	P.,	Casey,	M.	M.,	Galloway,	K.	W.,	Galloway,	R.	K.,	Kay,	A.	E.,	...	&	McQueen,	H.	A.
            (2014).	Student-generated	content:	enhancing	learning	through	sharing	multiple-choice
            questions.	International	Journal	of	Science	Education,	36(13),	2180-2194.
            King,	A.	(1992).	Facilitating	elaborative	learning	through	guided	student-generated	questioning.
            Educational	psychologist,	27(1),	111-126.
            Yu,	F.	Y.	(2009).	Scaffolding	student-generated	questions:	Design	and	development	of	a	customizable
            online	learning	system.	Computers	in	Human	Behavior,	25(5),	1129-1138.
          </p>
          <h1>References	for	the	Effects	of	Question-Writing	on	Exam	Performance</h1>
          <p>
            Bates,	S.	P.,	Galloway,	R.	K.,	&	McBride,	K.	L.	(2012,	February).	Student-generated	content:	Using
            PeerWise to	enhance	engagement	and	outcomes	in	introductory	physics	courses.	In	N.	S.	Rebello,
            P.	V.	Engelhardt,	&	C.	Singh	(Eds.),	AIP	Conference	Proceedings 1413(1),	123-126).	AIP.
            Van	Blerkom,	D.	L.,	Van	Blerkom,	M.	L.,	&	Bertsch,	S.	(2006).	Study	strategies	and	generative	learning:
            What	works?.	Journal	of	College	Reading	and	Learning,	37(1),	7-18.
            Carroll,	D.	W.	(2001).	Using	ignorance	questions	to	promote	thinking	skills.	Teaching	of	Psychology,	28(2),
            98-100.
            Dziuk,	E.	(2016).	Use	of	student-generated	questions	in	the	classroom.	Assessing	the	Assessor:	A	LearnerCentered
            Approach	to	Assessment,	10.
            Faize,	F.	A.,	&	Dahar,	M.	A.	(2012).	Engaging	secondary	grade	physics	students	in	developing	test	items.
            Journal	of	Turkish	Science	Education,	9(2).
            Feeley,	M.,	&	Parris,	J.	(2012).	An	assessment	of	the	PeerWise student-contributed	question	system's
            impact	on	learning	outcomes:	Evidence	from	a	large	enrollment	political	science	course.
            American	Political	Science	Association	Meeting	(unpublished	conference	paper),	New	Orleans,
            LA.
            Foos,	P.	W.	(1989).	Effects	of	student-written	questions	on	student	test	performance.	Teaching	of
            Psychology,	16(2),	77-78.
            Foos,	P.	W.,	Mora,	J.	J.,	&	Tkacz,	S.	(1994).	Student	study	techniques	and	the	generation	effect.	Journal	of
            Educational	Psychology,	86(4),	567.
            Foote,	C.	J.	(1998).	Student-generated	higher	order	questioning	as	a	study	strategy.	The	Journal	of
            Educational	Research,	92(2),	107-113.
            Frase,	L.	T.,	&	Schwartz,	B.	J.	(1975).	Effect	of	question	production	and	answering	on	prose	recall.	Journal
            of	Educational	Psychology, 67(5),	628.
          </p>
          <h1>References	for	the	Effects	of	Question-Writing	on	Exam	Performance	(Continued)</h1>
          <p>
            Green,	D.	H.	(1997).	Student-generated	exams:	Testing	and	learning.	Journal	of	Marketing	Education,
            19(2),	43-53.
            Kerkman,	D.	D.,	Kellison,	K.	L.,	Piñon,	M.	F.,	Schmidt,	D.,	&	Lewis,	S.	(1994).	The	quiz	game:	Writing	and
            explaining	questions	improve	quiz	scores.	Teaching	of	Psychology,	21(2),	104-106.
            King,	A.	(1991).	Effects	of	training	in	strategic	questioning	on	children's	problem-solving	performance.
            Journal	of	Educational	Psychology,	83(3),	307.[5:05]
            King,	A.,	&	Rosenshine,	B.	(1993).	Effects	of	guided	cooperative	questioning	on	children's	knowledge
            construction.	The	Journal	of	Experimental	Education,	61(2),	127-148.
            Luxton-Reilly,	A.	(2012).	The	design	and	evaluation	of	StudySieve:	A	tool	that	supports	student-generated
            free-response	questions,	answers	and	evaluations	(Doctoral	dissertation,	ResearchSpace
            Auckland).
            Luxton-Reilly,	A.,	Bertinshaw,	D.,	Denny,	P.,	Plimmer,	B.,	&	Sheehan,	R.	(2012,	February).	The	impact	of
            question	generation	activities	on	performance.	In	Proceedings	of	the	43rd	ACM	technical
            symposium	on	Computer	Science	Education (pp.	391-396).	ACM.
            McQueen,	H.	A.,	Shields,	C.,	Finnegan,	D.	J.,	Higham,	J.,	&	Simmen,	M.	W.	(2014).	Peerwise	provides
            significant	academic	benefits	to	biological	science	students	across	diverse	learning	tasks,	but	with
            minimal	instructor	intervention.	Biochemistry	and	Molecular	Biology	Education,	42(5),	371-381.
            Schullo-Feulner,	A.,	Janke,	K.	K.,	Chapman,	S.	A.,	Stanke,	L.,	Undeberg,	M.,	Taylor,	C.,	...	&	Straka,	R.	J.
            (2014).	Student-generated,	faculty-vetted	multiple-choice	questions:	Value,	participant
            satisfaction,	and	workload.	Currents	in	Pharmacy	Teaching	and	Learning,	6(1),	15-21.
            Song,	D.	(2016).	Student-generated	questioning	and	quality	questions:	A	literature	review.	Research
            Journal	of	Educational	Studies	and	Review,	2(5),	58-70.
            Yu,	F.	Y.,	&	Wu,	C.	P.	(2013).	Predictive	effects	of	online	peer	feedback	types	on	performance	quality.
            Educational	Technology	&	Society,	16(1),	332-341.
          </p>
        </div>
      </div>

    </template>

    <script type="module" src="prendus-research.ts"></script>
